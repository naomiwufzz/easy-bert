目录
1. [混合精度原理](#1-混合精度原理)
2. [实践](#2-实践)
   - [混合精度训练](#21-混合精度训练)
   - [半精度预测](#22-半精度预测)

## 1. 混合精度原理
torch里面**默认的浮点数是单精度的**，即float32。我们**可以将部分模型参数用float16，即fp16半精度来表示**，一来可以降低显存的占用，二来可以提升训练和推理的速度。

<br>

那么，**能不能将所有参数由fp32换成fp16**？

答案是**不能**，因为：
1. **fp16能表示的数值范围有限**（<img height="35" src="images/fp16-range.png" align="center"/>），和fp32的范围（<img height="35" src="images/fp32-range.png" align="center"/>
   ）相比要窄很多，在**计算的时候容易出现数值溢出**，从而产生`Nan`问题。
   - 模型的梯度一般较小，如果用fp16，容易出现下溢。
2. 梯度过小时，可能会产生**舍入误差**，导致参数更新失败。如下图例子：
   - <img height="35" src="images/rounding-error.png" align="center"/>

<br>

具体**混合精度训练如何进行**的呢？
1. 在内存中**用FP16做储存和乘法**从而加速计算，**用FP32做累加**避免舍入误差；
2. 为了防止梯度过小而下溢，**反向传播前将loss变化手动增大**`2^k`倍，**反向传播后将权重梯度减小**`2^k`倍；

<br>

更多理论参考：[【PyTorch】唯快不破：基于Apex的混合精度加速](https://zhuanlan.zhihu.com/p/79887894)

## 2. 实践
### 2.1 混合精度训练
torch1.6支持了混合精度训练，**代码样例**如下：

```python
from torch.cuda.amp import autocast, GradScaler

# 设置梯度缩放
self.grad_scaler = GradScaler()

# 用autocast包装model call
with autocast():
    best_paths, loss = self.model(batch_input_ids, batch_att_mask, labels=batch_label_ids)

# 按比例放大loss
loss = self.grad_scaler.scale(loss)

# 同比例缩小梯度，并更新参数
self.grad_scaler.step(self.optimizer)
self.grad_scaler.update()
```
**完整代码**参考：

- [easy_bert/bert4classification/classification_trainer.py](https://github.com/waking95/easy-bert/blob/main/easy_bert/bert4classification/classification_trainer.py)
- [easy_bert/bert4sequence_labeling/sequence_labeling_trainer.py](https://github.com/waking95/easy-bert/blob/main/easy_bert/bert4sequence_labeling/sequence_labeling_trainer.py)

### 2.2 半精度预测
推理的时候可以**直接使用半精度**，如下：

```python
self.model.half()
torch.cuda.empty_cache()  # 立即清空cache，降低显存占用
```

**完整代码**参考：
- [easy_bert/bert4classification/classification_predictor.py](https://github.com/waking95/easy-bert/blob/main/easy_bert/bert4classification/classification_predictor.py)
- [easy_bert/bert4sequence_labeling/sequence_labeling_predictor.py](https://github.com/waking95/easy-bert/blob/main/easy_bert/bert4sequence_labeling/sequence_labeling_predictor.py)

