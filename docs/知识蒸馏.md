
## 1. 蒸馏简介
基于transformer的模型比较重，硬件受限下，**如何提高推理速度**？

**知识蒸馏**（Knowledge Distillation）也是一种非常常用的方法

蒸馏一般包含**老师（Teacher）模型**和**学生（Student）模型**：
- **蒸馏目标**：用推理效率更高的、轻量的学生模型，近似达到老师的大模型的效果；
- 一般**老师的模型size（参数量）要大过学生**，比如用bert-large去教bert-base；
- 直接用学生模型去微调，满足不了我们的精度要求，因为学生模型容量较小；
- **蒸馏过程**：老师将知识（embedding/hidden/attention/logits等）教给学生；

接下来介绍3种**蒸馏过的预训练模型**：DistilBert、TinyBert、MobileBert

## 2. 蒸馏过的预训练模型——DistilBert
DistilBert是一个**6层的Bert**，由**12层的Bert-Base**当老师，在预训练阶段蒸馏得到

**预训练**时：
- DistilBert直接**使用老师的前6层初始化**；（因为参数的维度相同）
- **只进行了MLM任务**，没有进行NSP任务；
- 学生在学习时，除了**要利用真实的label**，还得**学习老师的隐层输出**（hidden）**和输出概率**（soft label）；

<img height="400" src="images/distil-bert.png"/>

**蒸馏的loss**定义为：

<img height="60" src="images/distil-bert-loss.png"/>

1. 第一项：**有监督MLM损失**
   - 被Mask的部分作为label，与学生输出计算交叉熵：<img height="50" src="images/distil-bert-loss-mlm.png" align="center"/>
2. 第二项：**蒸馏soft label损失**
   - 学生的输出`s_i`向老师的输出`t_i`看齐，两者计算交叉熵：<img height="50" src="images/distil-bert-loss-soft-label.png" align="center"/>
   - 蒸馏时，老师的输出`t_i`也称作**soft label**，它是logits经过softmax后的概率；
   - 这里的softmax函数一般带温度系数`T`，训练时设置`T=8`，推理时设置`T=1`：
   - <img height="50" src="images/distil-bert-loss-softmax.png" align="center"/>
3. 第三项：**输出层last hidden余弦损失**
   - 学生的last hidden `ℎ^t`向老师的last hidden `ℎ^s`看齐，计算余弦距离：<img height="50" src="images/distil-bert-loss-hidden.png" align="center"/>
